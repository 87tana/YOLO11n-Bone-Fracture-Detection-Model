# -*- coding: utf-8 -*-
"""NewDataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_3WzLynhH9X9GAOJaa5Vx__I7dHskSs8
"""

# Commented out IPython magic to ensure Python compatibility.
# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive/', force_remount=True)


# Navigate to the project directory
# %cd '/content/drive/MyDrive/Project_Experiments/Bone_Fraction_Detection/'


# Define dataset path
dataset_path = '/content/drive/MyDrive/Project_Experiments/Bone_Fraction_Detection/dataset/bonefractureyolo'

# Install necessary libraries
!pip install -q ultralytics torch torchvision opencv-python pillow matplotlib tqdm
!pip install tabulate

!pip install ultralytics torch torchvision torchaudio mlflow pyngrok pyyaml --quiet

# Import required libraries
import os
import random
import pandas as pd
import numpy as np
import yaml

from sklearn.cluster import KMeans
import cv2

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from IPython.display import display

from tqdm import tqdm
from ultralytics import YOLO
import ultralytics
from collections import Counter  # for counting class distribution
import time
import datetime
import subprocess


# Google Colab tools
from google.colab import drive



# MLflow
import mlflow
from mlflow.tracking import MlflowClient
from mlflow.entities import ViewType
import mlflow.pyfunc

# For ngrok to expose the MLflow UI remotely
#from pyngrok import ngrok

# Set dataset paths
dataset_path = '/content/drive/MyDrive/Project_Experiments/Bone_Fraction_Detection/dataset/bonefractureyolo'

train_images_dir = os.path.join(dataset_path, 'train/images')
train_labels_dir = os.path.join(dataset_path, 'train/labels')

validation_images_dir = os.path.join(dataset_path, 'valid/images')
validation_labels_dir = os.path.join(dataset_path, 'valid/labels')

test_images_dir = os.path.join(dataset_path, 'test/images')
test_labels_dir = os.path.join(dataset_path, 'test/labels')

#----------------------------------------------------------------------Train Dataset----------------------------------------------------------------

# list comprehensions to create a list for storing train images and labels respectively
train_image_files = sorted([f for f in os.listdir(train_images_dir) if f.endswith('.jpg' )])
train_label_files = sorted([f for f in os.listdir(train_labels_dir) if f.endswith('.txt')])


# Check if the number of images and labels match, ensure neither are empty, and verify consistency between image-label pairs.

if len(train_image_files) != len(train_label_files):
    print("Warning: Number of images and labels do not match.")
else:
    print("Train Dataset is consistent.")

# Quick summary of train set
print(f"Number of train images: {len(train_image_files)}")
print(f"Number of train labels: {len(train_label_files)}")

#----------------------------------------------------------------------Train Dataset----------------------------------------------------------------

# Filter and clean file names
def clean_filename(f):
    return os.path.splitext(f.strip().lower())[0]

train_image_files = sorted([f for f in os.listdir(train_images_dir)
                            if f.lower().endswith('.jpg') and not f.startswith('.')])
train_label_files = sorted([f for f in os.listdir(train_labels_dir)
                            if f.lower().endswith('.txt') and not f.startswith('.')])

if len(train_image_files) != len(train_label_files):
    print("Warning: Number of images and labels do not match.")
else:
    print("Train Dataset is consistent.")

print(f"Number of train images: {len(train_image_files)}")
print(f"Number of train labels: {len(train_label_files)}")

# Clean file names for comparison
image_names = {clean_filename(f) for f in train_image_files}
label_names = {clean_filename(f) for f in train_label_files}

if image_names != label_names:
    print("Mismatched files found:")
    print("Missing labels for images:", image_names - label_names)
    print("Missing images for labels:", label_names - image_names)
else:
    print("All image-label pairs match after cleaning.")

# ---------------------------------------------------------------------Valid Dataset----------------------------------------------------------------------------
# list comprehensions to create a list for storing valid images and labels respectively
validation_image_files = sorted([f for f in os.listdir(validation_images_dir) if f.endswith('.jpg')])
validation_label_files = sorted([f for f in os.listdir(validation_labels_dir) if f.endswith('.txt')])


# Check if the number of images and labels match, ensure neither are empty, and verify consistency between image-label pairs.

if len(validation_image_files) != len(validation_label_files):
    print("Warning: Number of images and labels do not match.")
else:
    print("Validation Dataset is consistent.")

# Quick summary of train set
print(f"Number of validation images: {len(validation_image_files)}")
print(f"Number of validation labels: {len(validation_label_files)}")

#----------------------------------------------------------------------- Test Dataset-----------------------------------------------------------------------------

# list comprehensions to create a list for storing valid images and labels respectively
test_image_files = sorted([f for f in os.listdir(test_images_dir) if f.endswith(('.jpg','.png'))])
test_label_files = sorted([f for f in os.listdir(test_labels_dir) if f.endswith('.txt')])


# Check if the number of images and labels match, ensure neither are empty, and verify consistency between image-label pairs.

if len(test_image_files) != len(test_label_files):
    print("Warning: Number of images and labels do not match.")
else:
    print("test Dataset is consistent.")

# Quick summary of train set
print(f"Number of test images: {len(test_image_files)}")
print(f"Number of test labels: {len(test_label_files)}")

import yaml

# Define the dataset path
dataset_path = '/content/drive/MyDrive/Project_Experiments/Bone_Fraction_Detection/dataset/bonefractureyolo'

# Construct the path to your YAML file
yaml_file_path = os.path.join(dataset_path, 'data.yaml')

# Load the YAML file
with open(yaml_file_path, 'r') as file:
    config = yaml.safe_load(file)

# Print the loaded configuration to verify
print(config)

"""## Class Distribution of each subset"""

# Define the dataset path
dataset_path = '/content/drive/MyDrive/Project_Experiments/Bone_Fraction_Detection/dataset/bonefractureyolo'

# Construct the path to your YAML file and load it
yaml_file_path = os.path.join(dataset_path, 'data.yaml')
with open(yaml_file_path, 'r') as file:
    config = yaml.safe_load(file)

# Extract class names from YAML; assumes keys "nc" and "names" exist
class_names_list = config['names']

# Create a mapping from class id (as string) to class name
class_map = {str(i): name for i, name in enumerate(class_names_list)}

# Construct paths for train images and labels
train_images_dir = os.path.join(dataset_path, 'train/images')
train_labels_dir = os.path.join(dataset_path, 'train/labels')

# Get sorted lists of image and label files
train_image_files = sorted([f for f in os.listdir(train_images_dir) if f.endswith('.jpg')])
train_label_files = sorted([f for f in os.listdir(train_labels_dir) if f.endswith('.txt')])

# Initialize a list to store all class ids from the label files
all_class_ids_training = []

# Loop through each label file and extract class ids
for label_file in train_label_files:
    label_path = os.path.join(train_labels_dir, label_file)
    with open(label_path, 'r') as f:
        for line in f:
            if line.strip():  # ensure the line is not empty
                parts = line.strip().split()
                class_id = parts[0]  # Assumes the first value is the class id
                all_class_ids_training.append(class_id)

# Count the frequency of each class id from your validation data
class_distribution = Counter(all_class_ids_training)

# Convert counts to lists for plotting and map numeric class IDs to class names
classes = list(class_distribution.keys())
counts = list(class_distribution.values())
mapped_class_names = [class_map.get(cid, cid) for cid in classes]

# Create a figure and plot the distribution
plt.figure(figsize=(12, 8))
bars = plt.bar(mapped_class_names, counts, color='blue')
plt.xlabel('Class')
plt.ylabel('Frequency')
plt.title('Distribution of Classes in Training Data')

# Annotate each bar with its frequency number
for bar in bars:
    height = bar.get_height()
    plt.annotate(f'{height}',
                 xy=(bar.get_x() + bar.get_width() / 2, height),
                 xytext=(0, 3),  # offset by 3 points above the bar
                 textcoords="offset points",
                 ha='center', va='bottom')

plt.show()

# Define the dataset path
dataset_path = '/content/drive/MyDrive/Project_Experiments/Bone_Fraction_Detection/dataset/bonefractureyolo'

# Construct the path to your YAML file and load it
yaml_file_path = os.path.join(dataset_path, 'data.yaml')
with open(yaml_file_path, 'r') as file:
    config = yaml.safe_load(file)

# Extract class names from YAML; assumes keys "nc" and "names" exist
class_names_list = config['names']

# Create a mapping from class id (as string) to class name
class_map = {str(i): name for i, name in enumerate(class_names_list)}

# Construct paths for validation images and labels
validation_images_dir = os.path.join(dataset_path, 'valid/images')
validation_labels_dir = os.path.join(dataset_path, 'valid/labels')

# Get sorted lists of image and label files
validation_image_files = sorted([f for f in os.listdir(validation_images_dir) if f.endswith('.jpg')])
validation_label_files = sorted([f for f in os.listdir(validation_labels_dir) if f.endswith('.txt')])

# Initialize a list to store all class ids from the label files
all_class_ids_validation = []

# Loop through each label file and extract class ids
for label_file in validation_label_files:
    label_path = os.path.join(validation_labels_dir, label_file)
    with open(label_path, 'r') as f:
        for line in f:
            if line.strip():  # ensure the line is not empty
                parts = line.strip().split()
                class_id = parts[0]  # Assumes the first value is the class id
                all_class_ids_validation.append(class_id)

# Count the frequency of each class id from your validation data
class_distribution = Counter(all_class_ids_validation)

# Convert counts to lists for plotting and map numeric class IDs to class names
classes = list(class_distribution.keys())
counts = list(class_distribution.values())
mapped_class_names = [class_map.get(cid, cid) for cid in classes]

# Create a figure and plot the distribution
plt.figure(figsize=(12, 8))
bars = plt.bar(mapped_class_names, counts, color='green')
plt.xlabel('Class')
plt.ylabel('Frequency')
plt.title('Distribution of Classes in Validation Data')

# Annotate each bar with its frequency number
for bar in bars:
    height = bar.get_height()
    plt.annotate(f'{height}',
                 xy=(bar.get_x() + bar.get_width() / 2, height),
                 xytext=(0, 3),  # offset by 3 points above the bar
                 textcoords="offset points",
                 ha='center', va='bottom')

plt.show()

# Define the dataset path
dataset_path = '/content/drive/MyDrive/Project_Experiments/Bone_Fraction_Detection/dataset/bonefractureyolo'

# Construct the path to your YAML file and load it
yaml_file_path = os.path.join(dataset_path, 'data.yaml')
with open(yaml_file_path, 'r') as file:
    config = yaml.safe_load(file)

# Extract class names from YAML; assumes keys "nc" and "names" exist
class_names_list = config['names']

# Create a mapping from class id (as string) to class name
class_map = {str(i): name for i, name in enumerate(class_names_list)}

# Construct paths for validation images and labels
test_images_dir = os.path.join(dataset_path, 'test/images')
test_labels_dir = os.path.join(dataset_path, 'test/labels')

# Get sorted lists of image and label files
test_image_files = sorted([f for f in os.listdir(test_images_dir) if f.endswith('.jpg')])
test_label_files = sorted([f for f in os.listdir(test_labels_dir) if f.endswith('.txt')])

# Initialize a list to store all class ids from the label files
all_class_ids_test = []

# Loop through each label file and extract class ids
for label_file in test_label_files:
    label_path = os.path.join(test_labels_dir, label_file)
    with open(label_path, 'r') as f:
        for line in f:
            if line.strip():  # ensure the line is not empty
                parts = line.strip().split()
                class_id = parts[0]  # Assumes the first value is the class id
                all_class_ids_test.append(class_id)

# Count the frequency of each class id from your test data
class_distribution = Counter(all_class_ids_test)

# Convert counts to lists for plotting and map numeric class IDs to class names
classes = list(class_distribution.keys())
counts = list(class_distribution.values())
mapped_class_names = [class_map.get(cid, cid) for cid in classes]

# Create a figure and plot the distribution
plt.figure(figsize=(12, 8))
bars = plt.bar(mapped_class_names, counts, color='pink')
plt.xlabel('Class')
plt.ylabel('Frequency')
plt.title('Distribution of Classes in test Data')

# Annotate each bar with its frequency number
for bar in bars:
    height = bar.get_height()
    plt.annotate(f'{height}',
                 xy=(bar.get_x() + bar.get_width() / 2, height),
                 xytext=(0, 3),  # offset by 3 points above the bar
                 textcoords="offset points",
                 ha='center', va='bottom')

plt.show()

# ====== Setup: Assume these are already computed ======
# train_distribution, val_distribution, test_distribution are Counters from your labels.
# For example:
train_distribution = Counter(all_class_ids_training)
val_distribution   = Counter(all_class_ids_validation)
test_distribution  = Counter(all_class_ids_test)

# And your YAML configuration has been loaded into config and class_map is defined as:
# class_map = {str(i): name for i, name in enumerate(config['names'])}

# Use the full set of class IDs based on the YAML file.
all_class_ids_fixed = [str(i) for i in range(len(config['names']))]

# Filter out classes that have zero count across all subsets.
filtered_class_ids = [
    cid for cid in all_class_ids_fixed
    if (train_distribution.get(cid, 0) > 0 or val_distribution.get(cid, 0) > 0 or test_distribution.get(cid, 0) > 0)
]

# Map the filtered class IDs to names.
mapped_class_names = [class_map[cid] for cid in filtered_class_ids]

# Re-calculate counts for each subset for only the filtered classes.
train_counts = [train_distribution.get(cid, 0) for cid in filtered_class_ids]
val_counts   = [val_distribution.get(cid, 0) for cid in filtered_class_ids]
test_counts  = [test_distribution.get(cid, 0) for cid in filtered_class_ids]

# ====== Plotting the grouped bar chart ======
x = np.arange(len(filtered_class_ids))  # the label locations
width = 0.25  # the width of the bars

plt.figure(figsize=(12,8))

# Create the grouped bars
bars_train = plt.bar(x - width, train_counts, width, label='Train', color='green')
bars_val   = plt.bar(x, val_counts, width, label='Validation', color='blue')
bars_test  = plt.bar(x + width, test_counts, width, label='Test', color='pink')

plt.xlabel('Class')
plt.ylabel('Frequency')
plt.title('Distribution of Classes across Datasets')
plt.xticks(x, mapped_class_names)
plt.legend()

# Annotate each bar with its frequency number
for bars in [bars_train, bars_val, bars_test]:
    for bar in bars:
        height = bar.get_height()
        plt.annotate(f'{height}',
                     xy=(bar.get_x() + bar.get_width() / 2, height),
                     xytext=(0, 3),  # offset above the bar
                     textcoords="offset points",
                     ha='center', va='bottom')

plt.tight_layout()
plt.show()

"""## Extract File name, image(H,W), bbx(h,w), Number of bbx"""

def parse_annotations(labels_dir, images_dir):
    """
    Extracts: file name, image dimensions, bounding boxes, and number of bounding boxes.
    Input: label_dir and image_dir.
    Output: A pandas DataFrame with details about each image and its bounding boxes.
    """

    data = []

    for label_file in tqdm(os.listdir(labels_dir)):  # Iterate over annotation files
        label_path = os.path.join(labels_dir, label_file)
        image_path = os.path.join(images_dir, label_file.replace('.txt', '.jpg'))  # Get corresponding image

        # Get image dimensions
        img = cv2.imread(image_path)
        if img is None:
            continue  # Skip if image is missing
        img_height, img_width = img.shape[:2]

        # Parse label file and extract bounding box information
        bboxes = []
        with open(label_path, 'r') as f:
            for line in f:
                tokens = line.strip().split()
                if len(tokens) < 5:
                    continue  # Skip if there aren't enough tokens
                # Only take the first 5 tokens in case there are extra values
                tokens = tokens[:5]
                _, x_center, y_center, width, height = map(float, tokens)
                # Convert relative dimensions to absolute pixels
                bbox_width = width * img_width
                bbox_height = height * img_height
                bboxes.append([bbox_width, bbox_height])

        data.append({
            "file_name": label_file,
            "img_width": img_width,
            "img_height": img_height,
            "bboxes": bboxes,
            "num_bboxes": len(bboxes)
        })

    # Convert the data to a pandas DataFrame
    df = pd.DataFrame(data)
    return df

validation_images_dir = os.path.join(dataset_path, 'valid/images')
validation_labels_dir = os.path.join(dataset_path, 'valid/labels')

val_data = parse_annotations(validation_labels_dir,validation_images_dir )

test_images_dir = os.path.join(dataset_path, 'test/images')
test_labels_dir = os.path.join(dataset_path, 'test/labels')

test_data = parse_annotations(test_labels_dir,test_images_dir )

train_images_dir = os.path.join(dataset_path, 'train/images')
train_labels_dir = os.path.join(dataset_path, 'train/labels')

train_data = parse_annotations(train_labels_dir,train_images_dir )

def plot_diagonal_for_all_subsets(train_data, test_data, val_data):
    """
    Plots the diagonal distributions for bounding boxes in the train, test, and validation subsets.

    Input:
    train_data, test_data, val_data: DataFrames containing parsed annotations for the respective subsets.
    """
    # Collect all the diagonals for each subset
    train_diagonals = []
    test_diagonals = []
    val_diagonals = []

    # Collect diagonals for the train set
    for _, row in train_data.iterrows():
        for bbox in row['bboxes']:
            # Calculate the diagonal for each bounding box
            diagonal = np.sqrt(bbox[0]**2 + bbox[1]**2)  # Pythagorean theorem
            train_diagonals.append(diagonal)

    # Collect diagonals for the test set
    for _, row in test_data.iterrows():
        for bbox in row['bboxes']:
            diagonal = np.sqrt(bbox[0]**2 + bbox[1]**2)
            test_diagonals.append(diagonal)

    # Collect diagonals for the validation set
    for _, row in val_data.iterrows():
        for bbox in row['bboxes']:
            diagonal = np.sqrt(bbox[0]**2 + bbox[1]**2)
            val_diagonals.append(diagonal)

    # Plot histograms for the diagonal distributions
    plt.figure(figsize=(10, 4))  # Adjust figure size for better comparability

    # Plot all subsets on the same plot
    plt.hist(train_diagonals, bins=15, alpha=0.9, color='aqua', edgecolor='black', label='Train Set')
    plt.hist(test_diagonals, bins=15, alpha=0.9, color='royalblue', edgecolor='black', label='Test Set')
    plt.hist(val_diagonals, bins=15, alpha=0.9, color='blue', edgecolor='black', label='Validation Set')

    # Adding titles and labels
    plt.title('Bounding Box Diagonal Distributions (All Subsets)')
    plt.xlabel('Diagonal (pixels)')
    plt.ylabel('Frequency')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Example Usage:
# Assuming train_data, test_data, and val_data are the DataFrames you get from parse_annotations for each subset.
plot_diagonal_for_all_subsets(train_data, test_data, val_data)

def plot_height_width_for_all_subsets(train_data, test_data, val_data):
    """
    Plots the height and width distributions for bounding boxes in the train, test, and validation subsets.

    Input:
    train_data, test_data, val_data: DataFrames containing parsed annotations for the respective subsets.
    """
    # Collect all the widths and heights for each subset
    train_widths = []
    train_heights = []
    test_widths = []
    test_heights = []
    val_widths = []
    val_heights = []

    for _, row in train_data.iterrows():
        for bbox in row['bboxes']:
            train_widths.append(bbox[0])
            train_heights.append(bbox[1])

    for _, row in test_data.iterrows():
        for bbox in row['bboxes']:
            test_widths.append(bbox[0])
            test_heights.append(bbox[1])

    for _, row in val_data.iterrows():
        for bbox in row['bboxes']:
            val_widths.append(bbox[0])
            val_heights.append(bbox[1])

    # Determine the common range for x-axis (dimension in pixels) based on min and max of all widths and heights
    min_dimension = min(min(train_widths), min(train_heights), min(test_widths), min(test_heights), min(val_widths), min(val_heights))
    max_dimension = max(max(train_widths), max(train_heights), max(test_widths), max(test_heights), max(val_widths), max(val_heights))

    # Plot histograms for the width and height distributions
    plt.figure(figsize=(15, 5))  # Adjust figure size for better comparability

    # Plot for train subset
    plt.subplot(1, 3, 1)
    plt.hist(train_widths, bins=15, alpha=0.7, label="Width", color='darkblue', edgecolor='black')
    plt.hist(train_heights, bins=15, alpha=0.7, label="Height", color='palegreen', edgecolor='black')
    plt.title('Train Set Bounding Box Dimensions')
    plt.xlabel('Dimension (pixels)')
    plt.ylabel('Frequency')
    plt.xlim(min_dimension, max_dimension)  # Use same x-axis limits for comparability
    plt.legend()

    # Plot for test subset
    plt.subplot(1, 3, 2)
    plt.hist(test_widths, bins=15, alpha=0.7, label="Width", color='darkblue', edgecolor='black')
    plt.hist(test_heights, bins=15, alpha=0.7, label="Height", color='palegreen', edgecolor='black')
    plt.title('Test Set Bounding Box Dimensions')
    plt.xlabel('Dimension (pixels)')
    plt.ylabel('Frequency')
    plt.xlim(min_dimension, max_dimension)  # Same x-axis limits for comparability
    plt.legend()

    # Plot for validation subset
    plt.subplot(1, 3, 3)
    plt.hist(val_widths, bins=15, alpha=0.7, label="Width", color='darkblue', edgecolor='black')
    plt.hist(val_heights, bins=15, alpha=0.7, label="Height", color='palegreen', edgecolor='black')
    plt.title('Validation Set Bounding Box Dimensions')
    plt.xlabel('Dimension (pixels)')
    plt.ylabel('Frequency')
    plt.xlim(min_dimension, max_dimension)  # Same x-axis limits for comparability
    plt.legend()

    plt.tight_layout()
    plt.show()


# Assuming train_data, test_data, and val_data are the DataFrames you get from parse_annotations for each subset.
plot_height_width_for_all_subsets(train_data, test_data, val_data)

"""# class conversion, make them one class"""

def class_conversion(in_dir, out_dir, txt_file):
  boxes = []
  with open(os.path.join(in_dir,txt_file), 'r') as f:
          for line in f:
              data = line.strip().split()
              class_id = int(data[0])
              if class_id!=0:
                data[0] = '0'
                boxes.append(data)

  with open(os.path.join(out_dir,txt_file), 'w') as f_out:
      for box in boxes:
          f_out.write(' '.join(map(str, box)) + '\n')

in_dir = os.path.join(dataset_path, "test/labels")
out_dir = os.path.join(dataset_path, "test/labels_conv")

for txt_file in sorted([f for f in os.listdir(in_dir) if f.endswith('.txt')]):
  class_conversion(in_dir, out_dir, txt_file)

"""# train"""

def train_yolo(model_path, dataset_path, log_path, exp_id=datetime.datetime.now().strftime('%Y%m%d%H%M%S'),
               epochs=50, lr=0.001, batch_size=32, optimizer='AdamW', resume=False):

    with mlflow.start_run(run_name=exp_id):
        # Log hyperparameters
        mlflow.log_param("epochs", epochs)
        mlflow.log_param("learning_rate", lr)
        mlflow.log_param("optimizer", optimizer)
        mlflow.log_param("img_size", 640)
        mlflow.log_param("batch_size", 64)

        # Start timer for training duration
        start_time = time.time()

        if resume:
            # Load a model
            model = YOLO(model_path)  # load a partially trained model
            # Resume training
            results = model.train(resume=True)
        else:
            # Hyperparameters for training YOLOv11n
            params = {
                'exp_id': exp_id,
                'epochs': epochs,
                'img_size': 640,
                'batch_size': batch_size,
                'lr': lr,
                'optimizer': optimizer,
                'save_period': 10,
            }

            # Load the YOLOv11n model
            model = YOLO(model_path)  # Load YOLOv11n or YOLOv11s model with custom weights

            # Train the model with updated parameters, including aggressive geometric and photometric augmentations
            results = model.train(
                                  cache= 'disk', # Cache data on disc to speed up training.
                                  plots=True,
                                  data=dataset_path,
                                  epochs=params['epochs'],
                                  imgsz=params['img_size'],
                                  batch=params['batch_size'],
                                  lr0=params['lr'],
                                  optimizer=params['optimizer'],
                                  project=log_path,
                                  name=params['exp_id'],
                                  save_period=params['save_period'],
                                  single_cls=True,
                                  close_mosaic=10,
                                  multi_scale=False,
                                  dfl=2.5,
                                  # Photometric augmentations (increased contrast/brightness adjustments)
                                  #mosaic=False,
                                  #hsv_h=0.0,      # Remove hue variation for grayscale images
                                  #hsv_s=0.0,      # Remove saturation adjustment for grayscale images
                                  #hsv_v=1.0,        # Maintain brightness adjustment
                                  #contrast=1.2,    # Set contrast to 1.2 for a vivid, increased contrast effect --> not valid
                                  # Geometric augmentations (more aggressive than default)
                                  degrees=90.0,    # Increased rotation range
                                  #translate=0.2,   # Increased translation (up to 30% shift)
                                  scale=0.9,       # Increased scaling variation (up to 90% of original size)
                                  shear=5.0,       # Increased shearing
                                  #perspective=0.0, # No perspective transform (can adjust if needed)
                                  # Optionally, you can also adjust mixup, flips, etc.
                                  mixup=0.1,
                                  flipud=0.5
                                  #fliplr=0.5
                              )

        # Log training time
        training_duration = time.time() - start_time
        mlflow.log_metric("training_duration", training_duration)
        print(f"Training completed in {training_duration:.2f} seconds")

        return results