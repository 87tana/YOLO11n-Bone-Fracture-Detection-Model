# -*- coding: utf-8 -*-
"""EDA_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d5FgpokA_DBhRE6UDu9Qnh7Fl-oZRlI4

in this notebook, i assess the label distribution, bounding box statistic, image dimension and data quality
"""

# Commented out IPython magic to ensure Python compatibility.
# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive/', force_remount=True)


# Navigate to the project directory
# %cd '/content/drive/MyDrive/Project_Experiments/Bone_Fraction_Detection/'


# Define dataset path
dataset_path = '/content/drive/MyDrive/Project_Experiments/Bone_Fraction_Detection/Fraction_Detection_Dataset'

# Install necessary libraries
!pip install -q ultralytics torch torchvision opencv-python pillow matplotlib tqdm
!pip install tabulate

# Import required libraries
import os
import random
import pandas as pd
import numpy as np

from sklearn.cluster import KMeans
import cv2

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from IPython.display import display

from tqdm import tqdm

from ultralytics import YOLO

from collections import Counter  # for counting class distribution

# Set dataset paths
dataset_path = '/content/drive/MyDrive/Project_Experiments/Bone_Fraction_Detection/Fraction_Detection_Dataset'

train_images_dir = os.path.join(dataset_path, 'train/images')
train_labels_dir = os.path.join(dataset_path, 'train/labels')

validation_images_dir = os.path.join(dataset_path, 'valid/images')
validation_labels_dir = os.path.join(dataset_path, 'valid/labels')

test_images_dir = os.path.join(dataset_path, 'test/images')
test_labels_dir = os.path.join(dataset_path, 'test/labels')

#----------------------------------------------------------------------Train Dataset----------------------------------------------------------------

# list comprehensions to create a list for storing train images and labels respectively
train_image_files = sorted([f for f in os.listdir(train_images_dir) if f.endswith('.jpg')])
train_label_files = sorted([f for f in os.listdir(train_labels_dir) if f.endswith('.txt')])


# Check if the number of images and labels match, ensure neither are empty, and verify consistency between image-label pairs.

if len(train_image_files) != len(train_label_files):
    print("Warning: Number of images and labels do not match.")
else:
    print("Train Dataset is consistent.")

# Quick summary of train set
print(f"Number of train images: {len(train_image_files)}")
print(f"Number of train labels: {len(train_label_files)}")

# ---------------------------------------------------------------------Valid Dataset----------------------------------------------------------------------------
# list comprehensions to create a list for storing valid images and labels respectively
validation_image_files = sorted([f for f in os.listdir(validation_images_dir) if f.endswith('.jpg')])
validation_label_files = sorted([f for f in os.listdir(validation_labels_dir) if f.endswith('.txt')])


# Check if the number of images and labels match, ensure neither are empty, and verify consistency between image-label pairs.

if len(validation_image_files) != len(validation_label_files):
    print("Warning: Number of images and labels do not match.")
else:
    print("Validation Dataset is consistent.")

# Quick summary of train set
print(f"Number of validation images: {len(validation_image_files)}")
print(f"Number of validation labels: {len(validation_label_files)}")

#----------------------------------------------------------------------- Test Dataset-----------------------------------------------------------------------------

# list comprehensions to create a list for storing valid images and labels respectively
test_image_files = sorted([f for f in os.listdir(test_images_dir) if f.endswith(('.jpg','.png'))])
test_label_files = sorted([f for f in os.listdir(test_labels_dir) if f.endswith('.txt')])


# Check if the number of images and labels match, ensure neither are empty, and verify consistency between image-label pairs.

if len(test_image_files) != len(test_label_files):
    print("Warning: Number of images and labels do not match.")
else:
    print("test Dataset is consistent.")

# Quick summary of train set
print(f"Number of test images: {len(test_image_files)}")
print(f"Number of test labels: {len(test_label_files)}")

"""# Extract File name, image(H,W), bbx(h,w), Number of bbx"""

def parse_annotations(labels_dir, images_dir):
    """
    Extracts: file name, image(W,H), bbx(W,H), Number of bbx
    Input: label_dir and image_dir
    Output: A list of dictionaries, each containing details about an image and its bounding boxes.
    """

    data = []

    for label_file in tqdm(os.listdir(labels_dir)):  # Iterate over annotation files
        label_path = os.path.join(labels_dir, label_file)
        image_path = os.path.join(images_dir, label_file.replace('.txt', '.jpg'))  # Get corresponding image

        # Get image dimensions
        img = cv2.imread(image_path)
        if img is None:
            continue  # Skip if image is missing
        img_height, img_width = img.shape[:2]

        # Parse label file and extract bounding box information
        bboxes = []
        with open(label_path, 'r') as f:
            for line in f:
                _, x_center, y_center, width, height = map(float, line.strip().split())  # Ignore class ID
                # Convert relative dimensions to absolute pixels
                bbox_width = width * img_width
                bbox_height = height * img_height
                bboxes.append([bbox_width, bbox_height])


        data.append({
            "file_name": label_file,
            "img_width": img_width,
            "img_height": img_height,
            "bboxes": bboxes,
            "num_bboxes": len(bboxes)
        })


    # Convert the data to a pandas DataFrame
    df = pd.DataFrame(data)
    return df

validation_images_dir = os.path.join(dataset_path, 'valid/images')
validation_labels_dir = os.path.join(dataset_path, 'valid/labels')

val_data = parse_annotations(validation_labels_dir,validation_images_dir )

# Display the DataFrame in a styled table format
display(val_data.head())

test_images_dir = os.path.join(dataset_path, 'test/images')
test_labels_dir = os.path.join(dataset_path, 'test/labels')

test_data = parse_annotations(test_labels_dir,test_images_dir )

# Display the DataFrame in a styled table format
display(test_data.head())

train_images_dir = os.path.join(dataset_path, 'train/images')
train_labels_dir = os.path.join(dataset_path, 'train/labels')

train_data = parse_annotations(train_labels_dir,train_images_dir )

# Display the DataFrame in a styled table format
display(train_data.tail())

def plot_height_width_for_all_subsets(train_data, test_data, val_data):
    """
    Plots the height and width distributions for bounding boxes in the train, test, and validation subsets.

    Input:
    train_data, test_data, val_data: DataFrames containing parsed annotations for the respective subsets.
    """
    # Collect all the widths and heights for each subset
    train_widths = []
    train_heights = []
    test_widths = []
    test_heights = []
    val_widths = []
    val_heights = []

    for _, row in train_data.iterrows():
        for bbox in row['bboxes']:
            train_widths.append(bbox[0])
            train_heights.append(bbox[1])

    for _, row in test_data.iterrows():
        for bbox in row['bboxes']:
            test_widths.append(bbox[0])
            test_heights.append(bbox[1])

    for _, row in val_data.iterrows():
        for bbox in row['bboxes']:
            val_widths.append(bbox[0])
            val_heights.append(bbox[1])

    # Determine the common range for x-axis (dimension in pixels) based on min and max of all widths and heights
    min_dimension = min(min(train_widths), min(train_heights), min(test_widths), min(test_heights), min(val_widths), min(val_heights))
    max_dimension = max(max(train_widths), max(train_heights), max(test_widths), max(test_heights), max(val_widths), max(val_heights))

    # Plot histograms for the width and height distributions
    plt.figure(figsize=(15, 5))  # Adjust figure size for better comparability

    # Plot for train subset
    plt.subplot(1, 3, 1)
    plt.hist(train_widths, bins=15, alpha=0.7, label="Width", color='darkblue', edgecolor='black')
    plt.hist(train_heights, bins=15, alpha=0.7, label="Height", color='palegreen', edgecolor='black')
    plt.title('Train Set Bounding Box Dimensions')
    plt.xlabel('Dimension (pixels)')
    plt.ylabel('Frequency')
    plt.xlim(min_dimension, max_dimension)  # Use same x-axis limits for comparability
    plt.legend()

    # Plot for test subset
    plt.subplot(1, 3, 2)
    plt.hist(test_widths, bins=15, alpha=0.7, label="Width", color='darkblue', edgecolor='black')
    plt.hist(test_heights, bins=15, alpha=0.7, label="Height", color='palegreen', edgecolor='black')
    plt.title('Test Set Bounding Box Dimensions')
    plt.xlabel('Dimension (pixels)')
    plt.ylabel('Frequency')
    plt.xlim(min_dimension, max_dimension)  # Same x-axis limits for comparability
    plt.legend()

    # Plot for validation subset
    plt.subplot(1, 3, 3)
    plt.hist(val_widths, bins=15, alpha=0.7, label="Width", color='darkblue', edgecolor='black')
    plt.hist(val_heights, bins=15, alpha=0.7, label="Height", color='palegreen', edgecolor='black')
    plt.title('Validation Set Bounding Box Dimensions')
    plt.xlabel('Dimension (pixels)')
    plt.ylabel('Frequency')
    plt.xlim(min_dimension, max_dimension)  # Same x-axis limits for comparability
    plt.legend()

    plt.tight_layout()
    plt.show()


# Assuming train_data, test_data, and val_data are the DataFrames you get from parse_annotations for each subset.
plot_height_width_for_all_subsets(train_data, test_data, val_data)

def plot_diagonal_for_all_subsets(train_data, test_data, val_data):
    """
    Plots the diagonal distributions for bounding boxes in the train, test, and validation subsets.

    Input:
    train_data, test_data, val_data: DataFrames containing parsed annotations for the respective subsets.
    """
    # Collect all the diagonals for each subset
    train_diagonals = []
    test_diagonals = []
    val_diagonals = []

    # Collect diagonals for the train set
    for _, row in train_data.iterrows():
        for bbox in row['bboxes']:
            # Calculate the diagonal for each bounding box
            diagonal = np.sqrt(bbox[0]**2 + bbox[1]**2)  # Pythagorean theorem
            train_diagonals.append(diagonal)

    # Collect diagonals for the test set
    for _, row in test_data.iterrows():
        for bbox in row['bboxes']:
            diagonal = np.sqrt(bbox[0]**2 + bbox[1]**2)
            test_diagonals.append(diagonal)

    # Collect diagonals for the validation set
    for _, row in val_data.iterrows():
        for bbox in row['bboxes']:
            diagonal = np.sqrt(bbox[0]**2 + bbox[1]**2)
            val_diagonals.append(diagonal)

    # Plot histograms for the diagonal distributions
    plt.figure(figsize=(10, 4))  # Adjust figure size for better comparability

    # Plot all subsets on the same plot
    plt.hist(train_diagonals, bins=15, alpha=0.6, color='aqua', edgecolor='black', label='Train Set')
    plt.hist(test_diagonals, bins=15, alpha=0.6, color='royalblue', edgecolor='black', label='Test Set')
    plt.hist(val_diagonals, bins=15, alpha=0.6, color='blue', edgecolor='black', label='Validation Set')

    # Adding titles and labels
    plt.title('Bounding Box Diagonal Distributions (All Subsets)')
    plt.xlabel('Diagonal (pixels)')
    plt.ylabel('Frequency')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Example Usage:
# Assuming train_data, test_data, and val_data are the DataFrames you get from parse_annotations for each subset.
plot_diagonal_for_all_subsets(train_data, test_data, val_data)

"""Train:
- A broad distribution with a peak around smaller diagonals (100–200 pixels).
- Includes a notable number of large diagonals, showing diversity in object sizes.

Test and validation:

- Sharper peaks at smaller diagonals, emphasizing that smaller bounding boxes dominate these subsets.
- Larger diagonals are less frequent than in the train set.

# KMeans Clustering on Bounding Box Sizes (Width, Height)
"""

def cluster_bounding_boxes(train_data, n_clusters=9):
    """
    Perform KMeans clustering on bounding box sizes (width, height) for the training set
    and visualize the results.

    Parameters:
    train_data: DataFrame containing the parsed annotations for the training set.
    n_clusters: Number of clusters for KMeans.
    """
    # Collect all bounding box widths and heights
    train_bboxes = []
    for _, row in train_data.iterrows():
        for bbox in row['bboxes']:
            train_bboxes.append(bbox)  # bbox = [width, height]

    train_bboxes = np.array(train_bboxes)

    # Perform KMeans clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(train_bboxes)
    cluster_centers = kmeans.cluster_centers_
    labels = kmeans.labels_

    # Plot the clusters
    plt.figure(figsize=(10, 6))
    for i in range(n_clusters):
        cluster_points = train_bboxes[labels == i]
        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i+1}', alpha=0.6)

    # Plot cluster centers
    plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], color='black', marker='X', s=200, label='Centroids')
    plt.title(f"KMeans Clustering of Bounding Boxes (n_clusters={n_clusters})")
    plt.xlabel("Width (pixels)")
    plt.ylabel("Height (pixels)")
    plt.legend()
    plt.grid(True)
    plt.show()

    return cluster_centers

# Example Usage:
# Assuming train_data is the DataFrame you get from parse_annotations for the training set
n_clusters = 9  # You can adjust this based on the number of anchors you plan to use
cluster_centers = cluster_bounding_boxes(train_data, n_clusters=n_clusters)
print("Cluster Centers (Width, Height):")
print(cluster_centers)

"""'dislocated_thumb_jpg.rf.4d8985eb9ad5cb714724e23021c4fc05.txt'  --> 3 bbx with x

film-x-ray-shoulder-radiograph-show-aneurysmal-bone-cyst-disease-which-benign-tumor-of-bone_jpg.rf.09949085f25199561ad05680434d0e1a.txt
"""

#label_file = '8644c7aa15b332130711e15a541800_gallery_jpg.rf.f8fc0e4e738a5462a01f354de1d63b5e.txt'
label_file = 'An-x-ray-showing-the-metacarpal-bone-fracture-at-right-fifth-bone-of-the-patient-on-the_png.rf.708160347dbf9f9316b0c5fcb25ab89e.txt'

label_name = os.path.splitext(label_file)[0]
image_path = os.path.join(train_images_dir, f"{label_name}.jpg")
vis_image = cv2.imread(image_path)
plt.imshow(vis_image)
plt.axis('off')
plt.show()

# Read the single YOLO label
label_path = os.path.join(train_labels_dir, label_file)

with open(label_path, 'r') as f:
    label = f.readline().strip().split()
    class_id, center_x, center_y, width, height = map(float, label)

# Get image dimensions
img_h, img_w, _ = vis_image.shape

# Convert YOLO relative coordinates to pixel coordinates
x1 = int((center_x - width / 2) * img_w)
y1 = int((center_y - height / 2) * img_h)
x2 = int((center_x + width / 2) * img_w)
y2 = int((center_y + height / 2) * img_h)

# Draw the rectangle (bounding box) on the image
cv2.rectangle(vis_image, (x1, y1), (x2, y2), (0, 255, 0), 2)

# Optionally, put class name or id on the image
cv2.putText(vis_image, f"Class {int(class_id)}", (x1, y1 - 10),
            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# Convert BGR to RGB for displaying with Matplotlib
vis_image_rgb = cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB)

# Display the image with bounding boxes
plt.imshow(vis_image_rgb)
plt.show()

"""## multiple bbx and visualize them"""

# The label file and corresponding image name
label_file = 'dislocated_thumb_jpg.rf.4d8985eb9ad5cb714724e23021c4fc05.txt'
image_name = os.path.splitext(label_file)[0] + '.jpg'  # Convert .txt to .jpg
image_path = os.path.join(train_images_dir, image_name)

# Read the image
vis_image = cv2.imread(image_path)

# Read the YOLO label file
label_path = os.path.join(train_labels_dir, label_file)

with open(label_path, 'r') as f:
    labels = f.readlines()  # Read all bounding boxes from the label file

# Get image dimensions
img_h, img_w, _ = vis_image.shape

# Loop through each bounding box in the label file
for label in labels:
    parts = label.strip().split()  # Split the line into parts
    class_id, center_x, center_y, width, height = map(float, parts)  # Parse values

    # Convert YOLO relative coordinates to pixel coordinates
    x1 = int((center_x - width / 2) * img_w)  # Top-left x
    y1 = int((center_y - height / 2) * img_h)  # Top-left y
    x2 = int((center_x + width / 2) * img_w)  # Bottom-right x
    y2 = int((center_y + height / 2) * img_h)  # Bottom-right y

    # Draw the rectangle (bounding box) on the image
    cv2.rectangle(vis_image, (x1, y1), (x2, y2), (0, 255, 0), 2)

    # Optionally, put the class label on the image
    cv2.putText(vis_image, f"Class {int(class_id)}", (x1, y1 - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# Convert BGR to RGB for Matplotlib display
vis_image_rgb = cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB)

# Display the image with bounding boxes
plt.imshow(vis_image_rgb)
plt.axis('off')
plt.title(f"Image with Multiple Bounding Boxes: {label_file}")
plt.show()

train_hist = train_data.num_bboxes
val_hist = val_data.num_bboxes
test_hist = test_data.num_bboxes

# Plot the histograms
plt.figure(figsize=(8, 4))

plt.hist(train_hist, bins=range(train_hist.min(), train_hist.max() + 1), alpha=0.6, label='Train Data', color='blue')
plt.hist(val_hist, bins=range(val_hist.min(), val_hist.max() + 1), alpha=0.6, label='Validation Data', color='orange')
plt.hist(test_hist, bins=range(test_hist.min(), test_hist.max() + 1), alpha=0.6, label='Test Data', color='green')

# Add labels, legend, and title
plt.xlabel('Number of Bounding Boxes')
plt.ylabel('Frequency')
plt.title('Distribution of Number of Bounding Boxes (Histogram)')
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

val_data.num_bboxes.value_counts()

test_data.num_bboxes.value_counts()

"""#  class ID across all YOLO label files in"""

# Directory containing YOLO label files
test_labels_dir = os.path.join(dataset_path, 'test/labels')

# Count class IDs across all label files
class_distribution = Counter(
    line.split()[0]
    for file in os.listdir(test_labels_dir) if file.endswith(".txt")
    for line in open(os.path.join(test_labels_dir, file))
)

# Print the class distribution
print("Class Distribution:", dict(class_distribution))

count=0
files = [f for f in os.listdir(test_labels_dir) if f.endswith(".txt")]
for file in files:
  for line in open(os.path.join(test_labels_dir, file)):
      if int(line.split()[0])  == 1:
        count+=1


print(count)



# Directory containing YOLO label files
validation_labels_dir = os.path.join(dataset_path, 'valid/labels')

# Count class IDs across all label files
class_distribution = Counter(
    line.split()[0]
    for file in os.listdir(validation_labels_dir) if file.endswith(".txt")
    for line in open(os.path.join(validation_labels_dir, file))
)

# Print the class distribution
print("Class Distribution:", dict(class_distribution))

# Directory containing YOLO label files
train_labels_dir = os.path.join(dataset_path, 'train/labels')

# Count class IDs across all label files
class_distribution = Counter(
    line.split()[0]
    for file in os.listdir(train_labels_dir) if file.endswith(".txt")
    for line in open(os.path.join(train_labels_dir, file))
)

# Print the class distribution
print("Class Distribution:", dict(class_distribution))

# Set dataset paths
dataset_path = '/content/drive/MyDrive/Project_Experiments/Bone_Fraction_Detection/Fraction_Detection_Dataset'



validation_images_dir = os.path.join(dataset_path, 'valid/images')
validation_labels_dir = os.path.join(dataset_path, 'valid/labels')


# Select which dataset to use (here, training set)
image_dir = validation_images_dir
label_dir = validation_labels_dir

# Get list of image files (filtering for common image extensions)
image_extensions = [".jpg", ".jpeg", ".png", ".bmp"]
image_files = [f for f in os.listdir(image_dir)
               if os.path.splitext(f)[1].lower() in image_extensions]

# Select first 4 images
selected_image_files = image_files[:4]

# Create a 2x2 subplot grid
fig, axes = plt.subplots(2, 2, figsize=(10, 10))
axes = axes.flatten()  # Flatten to easily iterate over axes

# Process and display each of the 4 images
for ax, image_file in zip(axes, selected_image_files):
    image_path = os.path.join(image_dir, image_file)
    vis_image = cv2.imread(image_path)

    if vis_image is None:
        ax.set_title(f"Could not read {image_file}")
        ax.axis("off")
        continue

    # Build the corresponding label file path (assuming same base name with .txt extension)
    label_file = os.path.splitext(image_file)[0] + ".txt"
    label_path = os.path.join(label_dir, label_file)

    if not os.path.exists(label_path):
        ax.set_title(f"No label for {image_file}")
        ax.axis("off")
        continue

    # Get image dimensions
    img_h, img_w, _ = vis_image.shape

    # Read all bounding boxes from the label file (YOLO format)
    with open(label_path, 'r') as f:
        lines = f.readlines()

    for line in lines:
        parts = line.strip().split()
        if len(parts) != 5:
            continue  # Skip invalid lines

        # Extract YOLO parameters: class_id, center_x, center_y, width, height (relative coordinates)
        class_id, center_x, center_y, width, height = map(float, parts)

        # Convert YOLO relative coordinates to absolute pixel coordinates
        x1 = int((center_x - width / 2) * img_w)
        y1 = int((center_y - height / 2) * img_h)
        x2 = int((center_x + width / 2) * img_w)
        y2 = int((center_y + height / 2) * img_h)

        # Draw the bounding box and class label on the image
        cv2.rectangle(vis_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(vis_image, f"Class {int(class_id)}", (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Convert BGR to RGB for Matplotlib
    vis_image_rgb = cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB)

    # Display the image in the subplot
    ax.imshow(vis_image_rgb)
    ax.set_title(image_file)
    ax.axis("off")

plt.tight_layout()
plt.show()

import os
import random
import cv2
import matplotlib.pyplot as plt

# Set dataset paths
dataset_path = '/content/drive/MyDrive/Project_Experiments/Bone_Fraction_Detection/Fraction_Detection_Dataset'

validation_images_dir = os.path.join(dataset_path, 'valid/images')
validation_labels_dir = os.path.join(dataset_path, 'valid/labels')

# Select which dataset to use (here, validation set)
image_dir = validation_images_dir
label_dir = validation_labels_dir

# Get list of image files (filtering for common image extensions)
image_extensions = [".jpg", ".jpeg", ".png", ".bmp"]
image_files = [f for f in os.listdir(image_dir)
               if os.path.splitext(f)[1].lower() in image_extensions]

# Randomly select 4 images
selected_image_files = random.sample(image_files, 4)

# Create a 2x2 subplot grid
fig, axes = plt.subplots(2, 2, figsize=(10, 10))
axes = axes.flatten()  # Flatten to easily iterate over axes

# Set scale factor for resizing images (e.g., 0.3 for 30% of original size)
scale_factor = 0.3

# Process and display each of the 4 images
for ax, image_file in zip(axes, selected_image_files):
    image_path = os.path.join(image_dir, image_file)
    vis_image = cv2.imread(image_path)

    if vis_image is None:
        ax.set_title(f"Could not read {image_file}")
        ax.axis("off")
        continue

    # Build the corresponding label file path (assuming same base name with .txt extension)
    label_file = os.path.splitext(image_file)[0] + ".txt"
    label_path = os.path.join(label_dir, label_file)

    if not os.path.exists(label_path):
        ax.set_title(f"No label for {image_file}")
        ax.axis("off")
        continue

    # Get image dimensions
    img_h, img_w, _ = vis_image.shape

    # Read all bounding boxes from the label file (YOLO format)
    with open(label_path, 'r') as f:
        lines = f.readlines()

    for line in lines:
        parts = line.strip().split()
        if len(parts) != 5:
            continue  # Skip invalid lines

        # Extract YOLO parameters: class_id, center_x, center_y, width, height (relative coordinates)
        class_id, center_x, center_y, width, height = map(float, parts)

        # Convert YOLO relative coordinates to absolute pixel coordinates
        x1 = int((center_x - width / 2) * img_w)
        y1 = int((center_y - height / 2) * img_h)
        x2 = int((center_x + width / 2) * img_w)
        y2 = int((center_y + height / 2) * img_h)

        # Draw the bounding box and class label on the image
        cv2.rectangle(vis_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(vis_image, f"Class {int(class_id)}", (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Convert BGR to RGB for Matplotlib
    vis_image_rgb = cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB)

    # Resize the image to be smaller using the scale factor
    new_width = int(vis_image_rgb.shape[1] * scale_factor)
    new_height = int(vis_image_rgb.shape[0] * scale_factor)
    small_vis_image_rgb = cv2.resize(vis_image_rgb, (new_width, new_height))

    # Display the image in the subplot
    ax.imshow(small_vis_image_rgb)
    ax.set_title(image_file)
    ax.axis("off")

plt.tight_layout()
plt.show()

import os
import random
import cv2
import matplotlib.pyplot as plt

# Set dataset paths
dataset_path = '/content/drive/MyDrive/Project_Experiments/Bone_Fraction_Detection/Fraction_Detection_Dataset'

validation_images_dir = os.path.join(dataset_path, 'valid/images')
validation_labels_dir = os.path.join(dataset_path, 'valid/labels')

# Choose the validation set
image_dir = validation_images_dir
label_dir = validation_labels_dir

# List of valid image extensions
image_extensions = [".jpg", ".jpeg", ".png", ".bmp"]

# Get all image files from the validation images directory
image_files = [f for f in os.listdir(image_dir)
               if os.path.splitext(f)[1].lower() in image_extensions]

# Randomly select 4 images
num_images_to_display = 4
selected_image_files = random.sample(image_files, num_images_to_display)

# Create a 2×2 grid (2 rows, 2 columns)
fig, axes = plt.subplots(2, 2, figsize=(12, 12))
axes = axes.flatten()  # Flatten so we can iterate easily

# Scale factor for resizing images (e.g., 0.3 = 30% of original size)
scale_factor = 0.3

# Process and display each of the 4 images
for ax, image_file in zip(axes, selected_image_files):
    image_path = os.path.join(image_dir, image_file)
    vis_image = cv2.imread(image_path)

    if vis_image is None:
        ax.set_title(f"Could not read {image_file}")
        ax.axis("off")
        continue

    # Build corresponding label file path (YOLO .txt with same base name)
    label_file = os.path.splitext(image_file)[0] + ".txt"
    label_path = os.path.join(label_dir, label_file)

    if not os.path.exists(label_path):
        ax.set_title(f"No label for {image_file}")
        ax.axis("off")
        continue

    # Image dimensions
    img_h, img_w, _ = vis_image.shape

    # Read bounding boxes from the label file (YOLO format)
    with open(label_path, 'r') as f:
        lines = f.readlines()

    for line in lines:
        parts = line.strip().split()
        # YOLO format: class_id, center_x, center_y, width, height
        if len(parts) == 5:
            class_id, center_x, center_y, width, height = map(float, parts)

            # Convert YOLO relative coords to pixel coords
            x1 = int((center_x - width / 2) * img_w)
            y1 = int((center_y - height / 2) * img_h)
            x2 = int((center_x + width / 2) * img_w)
            y2 = int((center_y + height / 2) * img_h)

            # Draw bounding box
            cv2.rectangle(vis_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
            # Draw class label
            cv2.putText(vis_image, f"Class {int(class_id)}", (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Convert BGR to RGB for Matplotlib
    vis_image_rgb = cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB)

    # Resize the image to be smaller
    new_width = int(vis_image_rgb.shape[1] * scale_factor)
    new_height = int(vis_image_rgb.shape[0] * scale_factor)
    small_vis_image_rgb = cv2.resize(vis_image_rgb, (new_width, new_height))

    # Display in the subplot
    ax.imshow(small_vis_image_rgb)
    ax.set_title(image_file)
    ax.axis("off")

plt.tight_layout()

# Optionally save the figure to disk for later upload
# plt.savefig("sample_4_images.png", dpi=150, bbox_inches="tight")

plt.show()